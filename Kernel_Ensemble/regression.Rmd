---
title: "SVM Regression"
author: "Ethan Huynh and Ryan Gagliardi"
output:
  html_document:
    df_print: paged
  pdf_document: default
editor_options:
  chunk_output_type: inline
---

Load packages and data.

```{r}
library(e1071)
library(MASS)
df <- read.csv('diamonds.csv')
df$cut <- factor(df$cut)
df$color <- factor(df$color)
df$clarity <- factor(df$clarity)
```


### Divide into train, test, validate

Cut the dataset from 50000 rows to 10000 rows randomly
Then divide into train, test, and validate sets

```{r}
set.seed(6164)
i <- sample(1:nrow(df), 0.2*nrow(df), replace=FALSE)
df2 <- df[i,]

spec <- c(train=.6, test=.2, validate=.2)
i2 <- sample(cut(1:nrow(df2),nrow(df2)*cumsum(c(0,spec)), labels=names(spec)))
train <- df2[i2=="train",]
test <- df2[i2=="test",]
vald <- df2[i2=="validate",]
```


### Data exploration

Let's explore the data with R functions and plots.

```{r}
str(df)
dim(df)
head(df)
```


### Try linear regression

```{r}
lm1 <- lm(price~carat+cut+color+clarity, data=train)
pred <- predict(lm1, newdata=test)
cor_lm1 <- cor(pred, test$price)
mse_lm1 <- mean((pred-test$price)^2)
print(paste('correlation:', cor_lm1))
print(paste('mse:', mse_lm1))
plot(fitted(lm1), resid(lm1), main = "Multiple Linear Regression")
abline(0,0)
```


### Try a linear kernel

```{r}
svm1 <- svm(price~carat+cut+color+clarity, data=train, kernel="linear", cost=10, scale=TRUE)
summary(svm1)
pred <- predict(svm1, newdata=test)
cor_svm1 <- cor(pred, test$price)
mse_svm1 <- mean((pred - test$price)^2)
print(paste('correlation:', cor_svm1))
print(paste('mse:', mse_svm1))
```


### Tune

```{r}
tune_svm1 <- tune(svm, price~carat+cut+color+clarity, data=vald, kernel="linear"
                  ,ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm1)
```


### Evaluate on best linear svm

```{r}
pred <- predict(tune_svm1$best.model, newdata=test)
cor_svm1_tune <- cor(pred, test$price)
mse_svm1_tune <- mean((pred - test$price)^2)
print(paste('correlation:', cor_svm1_tune))
print(paste('mse:', mse_svm1_tune))
```


### Try a polynomial kernel

```{r}
svm2 <- svm(price~carat+cut+color+clarity, data=train, kernel="polynomial", cost=10, scale=TRUE)
summary(svm2)
pred <- predict(svm2, newdata=test)
cor_svm2 <- cor(pred, test$price)
mse_svm2 <- mean((pred - test$price)^2)
print(paste('correlation:', cor_svm2))
print(paste('mse:', mse_svm2))
```


### Try a radial kernel

```{r}
svm3 <- svm(price~carat+cut+color+clarity, data=train, kernel="radial", cost=10, gamma=1, scale=TRUE)
summary(svm3)
pred <- predict(svm3, newdata=test)
cor_svm3 <- cor(pred, test$price)
mse_svm3 <- mean((pred - test$price)^2)
print(paste('correlation:', cor_svm3))
print(paste('mse:', mse_svm3))
```


### Tune hyperperameters

```{r}
set.seed(6164)
tune.out <- tune(svm, price~carat+cut+color+clarity, data=vald, kernel="radial",
                 ranges=list(cost=c(0.1,1,10,100,1000),
                             gamma=c(0.5,1,2,3,4)))
summary(tune.out)
svm4 <- svm(price~carat+cut+color+clarity, data=train, kernel="radial", cost=100, gamma=0.5, scale=TRUE)
summary(svm4)
pred <- predict(svm4, newdata=test)
cor_svm4 <- cor(pred, test$price)
mse_svm4 <- mean((pred - test$price)^2)
print(paste('correlation:', cor_svm4))
print(paste('mse:', mse_svm4))
```


### Best method

Radial was the best method that was tried, however only marginally better than the others. The predictors are very correlated to the target in this case which means that no matter which method is used, a fantastic correlation will be generated. The data was situated in a way that made Radial slightly edge out the other options but the difference was so small that it hardly matters in reality.